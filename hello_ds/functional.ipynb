{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Functional API\n",
    "===============\n",
    "\n",
    "https://keras.io/guides/functional_api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    #from keras.models import Sequential\n",
    "    from keras import layers\n",
    "except ImportError as e:\n",
    "    print(f\"Error occurred while importing modules: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras.layers.Input is available for use.\n",
      "Shape of 'inputs': (None, 784)\n",
      "Data type of 'inputs': <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# Start by creating an input node:\n",
    "\n",
    "try:\n",
    "    inputs = layers.Input(shape=(784,))\n",
    "    print(\"keras.layers.Input is available for use.\")\n",
    "    print(f\"Shape of 'inputs': {inputs.shape}\")\n",
    "    print(f\"Data type of 'inputs': {inputs.dtype}\")\n",
    "except (NameError, AttributeError):\n",
    "    print(\"keras.layers.Input is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You create a new node in the graph of layers by calling a layer on this inputs object:\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add more layers to the graph of layers:\n",
    "x = layers.Dense(64, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output layer\n",
    "outputs = layers.Dense(10)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a graphic model\n",
    "\n",
    "import pydot\n",
    "keras.utils.plot_model(model, \"my_first_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#API overview: a first end-to-end example\n",
    "===============\n",
    "When passing data to the built-in training loops of a model, you should either use NumPy arrays (if your data is small and fits in memory) or tf.data.Dataset objects. In the next few paragraphs, we'll use the MNIST dataset as NumPy arrays, in order to demonstrate how to use optimizers, losses, and metrics.\n",
    "\n",
    "Let's consider the following model (here, we build in with the Functional API, but it could be a Sequential model or a subclassed model as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the typical end-to-end workflow looks like, consisting of:\n",
    "\n",
    "1. Training\n",
    "2. Validation on a holdout set generated from the original training data\n",
    "3. Evaluation on the test data\n",
    "\n",
    "We'll use MNIST data for this example.\n",
    "\n",
    "https://keras.io/guides/training_with_built_in_methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code loads the MNIST dataset using the load_data() function from keras.datasets.mnist. \n",
    "It returns four NumPy arrays: x_train, y_train, x_test, and y_test. \n",
    "x_train and x_test contain the images (pixel data) of handwritten digits, \n",
    "while y_train and y_test contain the corresponding labels (digits from 0 to 9).\n",
    "'''\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The images in x_train are reshaped from 28x28 matrices to a flat array of 784 elements. \n",
    "Each image is transformed into a 1D array of pixel values. \n",
    "The astype(\"float32\") converts the pixel values to 32-bit floating-point numbers, \n",
    "and then the pixel values are normalized by dividing them by 255. \n",
    "Normalizing the pixel values to the range [0, 1] helps in faster convergence of the neural network during training.\n",
    "The same preprocessing is applied to the test set, x_test, to reshape and normalize the pixel values.\n",
    "The labels y_train and y_test are converted to 32-bit floating-point numbers.\n",
    "This conversion is done to ensure consistency in data types during training and evaluation.\n",
    "'''\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 10,000 samples for validation\n",
    "\n",
    "'''\n",
    "The last 10,000 samples from the training set are separated to create a validation set. \n",
    "This validation set will be used during training to monitor the model's performance and prevent overfitting.\n",
    "'''\n",
    "\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 8., 6., ..., 5., 6., 8.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The next steps are to perform the compilation and training of a neural \n",
    "network model using TensorFlow and Keras for the MNIST dataset.\n",
    "The method compile configures the model for training. \n",
    "It specifies the optimizer, loss function, and evaluation metrics for the model\n",
    "'''\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "    \n",
    "# The RMSprop optimizer is used for updating the model's weights during training\n",
    "# The Sparse Categorical Crossentropy loss function is used for calculating the model's loss during training\n",
    "# The Sparse Categorical Accuracy metric is used to monitor the accuracy of the model during training\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>optimizer</b> is responsible for updating the model's weights and biases (also known as model's parameters) \n",
    "iteratively during training process to minimize the loss function. \n",
    "The optimization process involves finding the optimal set of model parameters\n",
    "that results in the smallest possible value of the loss function. \n",
    "This optimization is often performed through an iterative process known as gradient descent or its variants.\n",
    "During each iteration of the training process, the optimizer calculates the gradients of the loss function with respect \n",
    "to the model's parameters. These gradients indicate the direction and magnitude of the steepest increase in the loss function.\n",
    "The optimizer then uses these gradients to update the model's parameters in the opposite direction to reduce the loss. \n",
    "The magnitude of the update is controlled by a learning rate, which determines how much the model's \n",
    "parameters should change at each step.\n",
    "The optimization process continues for multiple epochs, during which the optimizer repeatedly updates the model's parameters, \n",
    "gradually improving its performance on the training data.\n",
    "The choice of optimizer can significantly impact the convergence speed and the quality of the model's final results. \n",
    "Some popular optimizers include Stochastic Gradient Descent (SGD), Adam, RMSprop, and others\n",
    "\n",
    "The <b>loss function</b>, also known as a cost function or objective function, is used to measure \n",
    "how well a model is performing on a given task or problem. \n",
    "It quantifies the difference between the predicted output of the model and the true target values in the training data. \n",
    "The choice of the loss function depends on the task type, such as classification or regression. \n",
    "Selecting an appropriate loss function is essential, as it guides the model to learn meaningful representations from the data.\n",
    "For example, in classification tasks, the cross-entropy loss (binary or categorical) is commonly used, \n",
    "while mean squared error (MSE) loss is used for regression tasks.\n",
    "The ultimate objective is to find a set of model parameters that result in the smallest possible loss, \n",
    "indicating the model's best performance on the task at hand.\n",
    "\n",
    "<b>Evaluation metrics</b> are used to measure the model's performance during and after training. \n",
    "These metrics are useful for assessing how well the model is performing on tasks such as \n",
    "classification or regression on the validation or test set. \n",
    "Common evaluation metrics vary depending on the type of machine learning task:\n",
    "<ul>\n",
    "<li>Classification tasks: Accuracy, precision, recall, F1-score, ROC-AUC, etc.\n",
    "</li>\n",
    "<li>Regression tasks: Mean Squared Error (MSE), Mean Absolute Error (MAE), R-squared, etc.\n",
    "</li>\n",
    "</ul> \n",
    "\n",
    "The main purposes of evaluation metrics are as follows:\n",
    "1. Model Comparison: Evaluation metrics allow you to compare the performance of different models \n",
    "and determine which one is better suited for a specific task. For example, in a binary classification task, \n",
    "you can compare the accuracy, precision, recall, and F1-score of multiple models \n",
    "to select the one with the best overall performance.\n",
    "2. Model Selection: Evaluation metrics help in selecting the best model from a set of candidate models. \n",
    "A model with higher accuracy or better performance on other evaluation metrics is usually chosen for deployment.\n",
    "3. Hyperparameter Tuning: Evaluation metrics are used during hyperparameter tuning \n",
    "to find the best combination of hyperparameters that maximize the model's performance.\n",
    "4. Early Stopping: During training, evaluation metrics are monitored to implement early stopping. \n",
    "If the model's performance on the validation dataset does not improve or starts to deteriorate, \n",
    "training can be stopped early to avoid overfitting and save computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.3603 - sparse_categorical_accuracy: 0.8945 - val_loss: 0.2123 - val_sparse_categorical_accuracy: 0.9354\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1647 - sparse_categorical_accuracy: 0.9508 - val_loss: 0.1853 - val_sparse_categorical_accuracy: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nx_train and y_train are the training data.\\nbatch_size=64: The batch size determines how many samples are processed together before updating the model's weights.\\nepochs=2: The number of epochs is the number of times the entire dataset is passed through the model during training.\\nvalidation_data=(x_val, y_val): The validation data is used to evaluate the model's performance during training.\\nif you already have a specific validation dataset that is different from the training data. \\nThis method gives more control over the validation process and allows you to use specific data for validation \\nthat is not part of the training data.\\nBut sometimes you can do it by using the validation_split argument.\\nIt is convenient when you want to quickly allocate a portion of the training data for validation without creating \\nseparate validation arrays specifying the fraction of the training data to use for validation, like as validation_split=0.2\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method trains the model on the provided training data\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
    "\n",
    "'''\n",
    "x_train and y_train are the training data.\n",
    "batch_size=64: The batch size determines how many samples are processed together before updating the model's weights.\n",
    "epochs=2: The number of epochs is the number of times the entire dataset is passed through the model during training.\n",
    "validation_data=(x_val, y_val): The validation data is used to evaluate the model's performance during training.\n",
    "if you already have a specific validation dataset that is different from the training data. \n",
    "This method gives more control over the validation process and allows you to use specific data for validation \n",
    "that is not part of the training data.\n",
    "But sometimes you can do it by using the validation_split argument.\n",
    "It is convenient when you want to quickly allocate a portion of the training data for validation without creating \n",
    "separate validation arrays specifying the fraction of the training data to use for validation, like as validation_split=0.2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.3603207468986511, 0.16468480229377747],\n",
       " 'sparse_categorical_accuracy': [0.8945249915122986, 0.9508000016212463],\n",
       " 'val_loss': [0.2123466283082962, 0.18529994785785675],\n",
       " 'val_sparse_categorical_accuracy': [0.9354000091552734, 0.9444000124931335]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The returned history object holds a record of the loss values and metric values during training:\n",
    "\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 - 0s - loss: 0.1520 - sparse_categorical_accuracy: 0.9537 - 205ms/epoch - 3ms/step\n",
      "Test loss: 0.1520078182220459\n",
      "Test accuracy: 0.9537000060081482\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, batch_size=128, verbose=2)\n",
    "\n",
    "''' \n",
    "Performs the evaluation of a trained machine learning model on a test dataset (x_test and y_test) \n",
    "to assess its performance on unseen data. The evaluate method is used to compute the test scores or performance metrics of the model.\n",
    "\n",
    "1. test_scores = model.evaluate(x_test, y_test, verbose=2): This line evaluates the trained model on the test dataset \n",
    "(x_test and y_test). The evaluate` method computes the loss and any specified evaluation metrics on the test data.\n",
    "\n",
    "2. test_scores: The evaluate method returns the test scores, which include the computed loss value and \n",
    "any specified evaluation metrics (e.g., accuracy, precision, recall, etc.). \n",
    "These scores indicate how well the model performs on the unseen test data.\n",
    "'''\n",
    "\n",
    "print(\"Test loss:\", test_scores[0]) # The test_scores[0] contains the test loss value, which is the computed value of the loss function on the test data\n",
    "print(\"Test accuracy:\", test_scores[1]) # The test_scores[1] contains the test accuracy value, which is the computed value of the accuracy metric on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for 3 samples\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Save and serialize\n",
    "The standard way to save a functional model is to call model.save() to save the entire model as a single file. You can later recreate the same model from this file, even if the code that built the model is no longer available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"path_to_my_model.keras\")\n",
    "del model\n",
    "# Recreate the exact same model purely from the file:\n",
    "model = keras.models.load_model(\"path_to_my_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Reusing models\n",
    "In the functional API, models are created by specifying their inputs and outputs in a graph of layers. That means that a single graph of layers can be used to generate multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 8, 8, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 16)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Encoder Part:\n",
    "\n",
    "'''\n",
    "\n",
    "# creates an input layer with the shape (28, 28, 1), representing grayscale images of size 28x28 pixels.\n",
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\") \n",
    "\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)  # applies a 2D convolutional layer with 16 filters of size 3x3 to the input.\n",
    "                                                            # The relu activation function is used.\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x) # Another 2D convolutional layer is applied with 32 filters of size 3x3 to the output of the previous layer\n",
    "x = layers.MaxPooling2D(3)(x) # A 2D max pooling layer with pool size 3x3 is applied to reduce the spatial dimensions of the data.\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x) #Another 2D convolutional layer is applied with 32 filters of size 3x3.\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x) # Another 2D convolutional layer is applied with 16 filters of size 3x3.\n",
    "\n",
    "# The output of the encoder is obtained by applying a global max pooling operation along the spatial dimensions of the data, \n",
    "# resulting in a tensor of shape (batch_size, 16).\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x) \n",
    "\n",
    "# This line defines the encoder model, which takes the encoder_input and outputs the encoder_output. \n",
    "# It creates an instance of the keras.Model class, which represents the encoder part of the autoencoder\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 8, 8, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 16)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 6, 6, 16)         160       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nThe code defines an autoencoder architecture, consisting of an encoder part followed by a decoder part. \\nThe autoencoder is a type of neural network used for unsupervised learning, \\nwhere it attempts to reconstruct the input data at its output, thus learning a compressed representation (latent space)\\n of the input data. The encoder part compresses the input data into a lower-dimensional representation, \\n and the decoder part tries to reconstruct the original data from this representation.\\n '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Decoder Part:\n",
    "\n",
    "decoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\n",
    "\n",
    "'''\n",
    "# Reshape layer allows us to change the dimensions of our input data. This is particularly useful when we need to prepare our data for specific types of layers or operations that require a certain input shape\n",
    "x = layers.Reshape((4, 4, 1))(encoder_output) # the Reshape layer will transform the encoder_output tensor into a shape of (4, 4, 1).\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x) # A 2D transposed convolutional layer (deconvolutional layer) with 16 filters of size 3x3 is applied.\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x) # Another 2D transposed convolutional layer with 32 filters of size 3x3 is applied.\n",
    "x = layers.UpSampling2D(3)(x) # An upsampling layer is applied, which increases the spatial dimensions of the data by a factor of 3.\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x) # Another 2D transposed convolutional layer with 16 filters of size 3x3 is applied.\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x) # The final layer applies a 2D transposed convolutional layer with 1 filter of size 3x3, \n",
    "                                                                    # which produces the decoder output.\n",
    "\n",
    "'''\n",
    "Autoencoder Model:\n",
    "\n",
    "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\"): \n",
    "This line defines the autoencoder model, which takes the encoder_input and outputs the decoder_output. \n",
    "It creates an instance of the keras.Model class, which represents the complete autoencoder model.\n",
    "'''\n",
    "\n",
    "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
    "autoencoder.summary()\n",
    "\n",
    "''' \n",
    "The code defines an autoencoder architecture, consisting of an encoder part followed by a decoder part. \n",
    "The autoencoder is a type of neural network used for unsupervised learning, \n",
    "where it attempts to reconstruct the input data at its output, thus learning a compressed representation (latent space)\n",
    " of the input data. The encoder part compresses the input data into a lower-dimensional representation, \n",
    " and the decoder part tries to reconstruct the original data from this representation.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " original_img (InputLayer)   [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoded_img (InputLayer)    [(None, 16)]              0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 6, 6, 16)         160       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,569\n",
      "Trainable params: 9,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                18672     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         9569      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In the example above, the stack of layers instantiate two models:\n",
    "- an encoder model that turns image inputs into 16-dimensional vectors, \n",
    "- and an end-to-end autoencoder model for training.\n",
    "\n",
    "Bellow, there's a different take on the autoencoder example that creates an encoder model, \n",
    "a decoder model, and chains them in two calls to obtain the autoencoder model:\n",
    "\n",
    "'''\n",
    "\n",
    "# 1st model\n",
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# 2nd model\n",
    "decoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\n",
    "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "encoded_img = encoder(autoencoder_input)\n",
    "decoded_img = decoder(encoded_img)\n",
    "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                18672     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         9569      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code defines an autoencoder model using the Keras functional API.\n",
    "In the given code, encoder(autoencoder_input) treats the encoder model as a function \n",
    "that takes the autoencoder_input tensor as input and produces the encoded_img tensor as output. \n",
    "It applies the encoder model to the input data to obtain the compressed representation in the latent space.\n",
    "\n",
    "The code defines an autoencoder architecture, consisting of an encoder part followed by a decoder part. \n",
    "The autoencoder is a type of neural network used for unsupervised learning, where it attempts to \n",
    "reconstruct the input data at its output, thus learning a compressed representation (latent space) of the input data. \n",
    "The encoder part compresses the input data into a lower-dimensional representation, and the decoder part tries to \n",
    "reconstruct the original data from this representation.\n",
    "\n",
    "'''\n",
    "\n",
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\") #creates an input layer for the autoencoder model.\n",
    "\n",
    "encoded_img = encoder(autoencoder_input)    # The code passes the autoencoder_input through the previously defined encoder model (encoder).\n",
    "                                            # This operation compresses the input images into a lower-dimensional representation (latent space) \n",
    "                                            # using the encoder.\n",
    "\n",
    "decoded_img = decoder(encoded_img)  # The code then takes the output of the encoder (encoded_img) \n",
    "                                    # and passes it through the previously defined decoder model (decoder). \n",
    "                                    # This operation reconstructs the original images from the compressed representation \n",
    "                                    # in the latent space using the decoder.\n",
    "\n",
    "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")   # The autoencoder takes the autoencoder_input and outputs \n",
    "                                                                                # the decoded_img, representing the reconstructed images. \n",
    "                                                                                # It creates an instance of the keras.Model \n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " original_img (InputLayer)   [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoded_img (InputLayer)    [(None, 16)]              0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 6, 6, 16)         160       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,569\n",
      "Trainable params: 9,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31012",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
