{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Functional API\n",
    "===============\n",
    "\n",
    "https://keras.io/guides/functional_api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    #from keras.models import Sequential\n",
    "    from keras import layers\n",
    "    from keras import models\n",
    "except ImportError as e:\n",
    "    print(f\"Error occurred while importing modules: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras.layers.Input is available for use.\n",
      "Shape of 'inputs': (None, 784)\n",
      "Data type of 'inputs': <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# Start by creating an input node:\n",
    "\n",
    "try:\n",
    "    inputs = layers.Input(shape=(784,))\n",
    "    print(\"keras.layers.Input is available for use.\")\n",
    "    print(f\"Shape of 'inputs': {inputs.shape}\")\n",
    "    print(f\"Data type of 'inputs': {inputs.dtype}\")\n",
    "except (NameError, AttributeError):\n",
    "    print(\"keras.layers.Input is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You create a new node in the graph of layers by calling a layer on this inputs object:\n",
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add more layers to the graph of layers:\n",
    "x = layers.Dense(64, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output layer\n",
    "outputs = layers.Dense(10)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"test_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a graphic model\n",
    "\n",
    "import pydot\n",
    "keras.utils.plot_model(model, \"functional_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#API overview: a first end-to-end example\n",
    "===============\n",
    "When passing data to the built-in training loops of a model, you should either use NumPy arrays (if your data is small and fits in memory) or tf.data.Dataset objects. In the next few paragraphs, we'll use the MNIST dataset as NumPy arrays, in order to demonstrate how to use optimizers, losses, and metrics.\n",
    "\n",
    "Let's consider the following model (here, we build in with the Functional API, but it could be a Sequential model or a subclassed model as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the typical end-to-end workflow looks like, consisting of:\n",
    "\n",
    "1. Training\n",
    "2. Validation on a holdout set generated from the original training data\n",
    "3. Evaluation on the test data\n",
    "\n",
    "We'll use MNIST data for this example.\n",
    "\n",
    "https://keras.io/guides/training_with_built_in_methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code loads the MNIST dataset using the load_data() function from keras.datasets.mnist. \n",
    "It returns four NumPy arrays: x_train, y_train, x_test, and y_test. \n",
    "x_train and x_test contain the images (pixel data) of handwritten digits, \n",
    "while y_train and y_test contain the corresponding labels (digits from 0 to 9).\n",
    "'''\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The images in x_train are reshaped from 28x28 matrices to a flat array of 784 elements. \n",
    "Each image is transformed into a 1D array of pixel values. \n",
    "The astype(\"float32\") converts the pixel values to 32-bit floating-point numbers, \n",
    "and then the pixel values are normalized by dividing them by 255. \n",
    "Normalizing the pixel values to the range [0, 1] helps in faster convergence of the neural network during training.\n",
    "The same preprocessing is applied to the test set, x_test, to reshape and normalize the pixel values.\n",
    "The labels y_train and y_test are converted to 32-bit floating-point numbers.\n",
    "This conversion is done to ensure consistency in data types during training and evaluation.\n",
    "'''\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve 10,000 samples for validation\n",
    "\n",
    "'''\n",
    "The last 10,000 samples from the training set are separated to create a validation set. \n",
    "This validation set will be used during training to monitor the model's performance and prevent overfitting.\n",
    "'''\n",
    "\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 8., 6., ..., 5., 6., 8.], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The next steps are to perform the compilation and training of a neural \n",
    "network model using TensorFlow and Keras for the MNIST dataset.\n",
    "The method compile configures the model for training. \n",
    "It specifies the optimizer, loss function, and evaluation metrics for the model\n",
    "'''\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "    \n",
    "# The RMSprop optimizer is used for updating the model's weights during training\n",
    "# The Sparse Categorical Crossentropy loss function is used for calculating the model's loss during training\n",
    "# The Sparse Categorical Accuracy metric is used to monitor the accuracy of the model during training\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>optimizer</b> is responsible for updating the model's weights and biases (also known as model's parameters) \n",
    "iteratively during training process to minimize the loss function. \n",
    "The optimization process involves finding the optimal set of model parameters\n",
    "that results in the smallest possible value of the loss function. \n",
    "This optimization is often performed through an iterative process known as gradient descent or its variants.\n",
    "During each iteration of the training process, the optimizer calculates the gradients of the loss function with respect \n",
    "to the model's parameters. These gradients indicate the direction and magnitude of the steepest increase in the loss function.\n",
    "The optimizer then uses these gradients to update the model's parameters in the opposite direction to reduce the loss. \n",
    "The magnitude of the update is controlled by a learning rate, which determines how much the model's \n",
    "parameters should change at each step.\n",
    "The optimization process continues for multiple epochs, during which the optimizer repeatedly updates the model's parameters, \n",
    "gradually improving its performance on the training data.\n",
    "The choice of optimizer can significantly impact the convergence speed and the quality of the model's final results. \n",
    "Some popular optimizers include Stochastic Gradient Descent (SGD), Adam, RMSprop, and others\n",
    "\n",
    "The <b>loss function</b>, also known as a cost function or objective function, is used to measure \n",
    "how well a model is performing on a given task or problem. \n",
    "It quantifies the difference between the predicted output of the model and the true target values in the training data. \n",
    "The choice of the loss function depends on the task type, such as classification or regression. \n",
    "Selecting an appropriate loss function is essential, as it guides the model to learn meaningful representations from the data.\n",
    "For example, in classification tasks, the cross-entropy loss (binary or categorical) is commonly used, \n",
    "while mean squared error (MSE) loss is used for regression tasks.\n",
    "The ultimate objective is to find a set of model parameters that result in the smallest possible loss, \n",
    "indicating the model's best performance on the task at hand.\n",
    "\n",
    "<b>Evaluation metrics</b> are used to measure the model's performance during and after training. \n",
    "These metrics are useful for assessing how well the model is performing on tasks such as \n",
    "classification or regression on the validation or test set. \n",
    "Common evaluation metrics vary depending on the type of machine learning task:\n",
    "<ul>\n",
    "<li>Classification tasks: Accuracy, precision, recall, F1-score, ROC-AUC, etc.\n",
    "</li>\n",
    "<li>Regression tasks: Mean Squared Error (MSE), Mean Absolute Error (MAE), R-squared, etc.\n",
    "</li>\n",
    "</ul> \n",
    "\n",
    "The main purposes of evaluation metrics are as follows:\n",
    "1. Model Comparison: Evaluation metrics allow you to compare the performance of different models \n",
    "and determine which one is better suited for a specific task. For example, in a binary classification task, \n",
    "you can compare the accuracy, precision, recall, and F1-score of multiple models \n",
    "to select the one with the best overall performance.\n",
    "2. Model Selection: Evaluation metrics help in selecting the best model from a set of candidate models. \n",
    "A model with higher accuracy or better performance on other evaluation metrics is usually chosen for deployment.\n",
    "3. Hyperparameter Tuning: Evaluation metrics are used during hyperparameter tuning \n",
    "to find the best combination of hyperparameters that maximize the model's performance.\n",
    "4. Early Stopping: During training, evaluation metrics are monitored to implement early stopping. \n",
    "If the model's performance on the validation dataset does not improve or starts to deteriorate, \n",
    "training can be stopped early to avoid overfitting and save computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.3690 - sparse_categorical_accuracy: 0.8968 - val_loss: 0.2239 - val_sparse_categorical_accuracy: 0.9319\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1672 - sparse_categorical_accuracy: 0.9499 - val_loss: 0.1671 - val_sparse_categorical_accuracy: 0.9520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nx_train and y_train are the training data.\\nbatch_size=64: The batch size determines how many samples are processed together before updating the model's weights.\\nepochs=2: The number of epochs is the number of times the entire dataset is passed through the model during training.\\nvalidation_data=(x_val, y_val): The validation data is used to evaluate the model's performance during training.\\nif you already have a specific validation dataset that is different from the training data. \\nThis method gives more control over the validation process and allows you to use specific data for validation \\nthat is not part of the training data.\\nBut sometimes you can do it by using the validation_split argument.\\nIt is convenient when you want to quickly allocate a portion of the training data for validation without creating \\nseparate validation arrays specifying the fraction of the training data to use for validation, like as validation_split=0.2\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method trains the model on the provided training data\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
    "\n",
    "'''\n",
    "x_train and y_train are the training data.\n",
    "batch_size=64: The batch size determines how many samples are processed together before updating the model's weights.\n",
    "epochs=2: The number of epochs is the number of times the entire dataset is passed through the model during training.\n",
    "validation_data=(x_val, y_val): The validation data is used to evaluate the model's performance during training.\n",
    "if you already have a specific validation dataset that is different from the training data. \n",
    "This method gives more control over the validation process and allows you to use specific data for validation \n",
    "that is not part of the training data.\n",
    "But sometimes you can do it by using the validation_split argument.\n",
    "It is convenient when you want to quickly allocate a portion of the training data for validation without creating \n",
    "separate validation arrays specifying the fraction of the training data to use for validation, like as validation_split=0.2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.36896899342536926, 0.16721712052822113],\n",
       " 'sparse_categorical_accuracy': [0.8968499898910522, 0.9498500227928162],\n",
       " 'val_loss': [0.22391797602176666, 0.16705213487148285],\n",
       " 'val_sparse_categorical_accuracy': [0.9319000244140625, 0.9520000219345093]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The returned history object holds a record of the loss values and metric values during training:\n",
    "\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.callbacks.History"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 - 0s - loss: 0.1395 - sparse_categorical_accuracy: 0.9582 - 310ms/epoch - 4ms/step\n",
      "Test loss: 0.13945473730564117\n",
      "Test accuracy: 0.9581999778747559\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, batch_size=128, verbose=2)\n",
    "\n",
    "''' \n",
    "Performs the evaluation of a trained machine learning model on a test dataset (x_test and y_test) \n",
    "to assess its performance on unseen data. The evaluate method is used to compute the test scores or performance metrics of the model.\n",
    "\n",
    "1. test_scores = model.evaluate(x_test, y_test, verbose=2): This line evaluates the trained model on the test dataset \n",
    "(x_test and y_test). The evaluate` method computes the loss and any specified evaluation metrics on the test data.\n",
    "\n",
    "2. test_scores: The evaluate method returns the test scores, which include the computed loss value and \n",
    "any specified evaluation metrics (e.g., accuracy, precision, recall, etc.). \n",
    "These scores indicate how well the model performs on the unseen test data.\n",
    "'''\n",
    "\n",
    "print(\"Test loss:\", test_scores[0]) # The test_scores[0] contains the test loss value, which is the computed value of the loss function on the test data\n",
    "print(\"Test accuracy:\", test_scores[1]) # The test_scores[1] contains the test accuracy value, which is the computed value of the accuracy metric on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions for 3 samples\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Save and serialize\n",
    "The standard way to save a functional model is to call model.save() to save the entire model as a single file. You can later recreate the same model from this file, even if the code that built the model is no longer available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"functional_model.keras\")\n",
    "del model\n",
    "# Recreate the exact same model purely from the file:\n",
    "model = keras.models.load_model(\"functional_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Reusing models\n",
    "In the functional API of Keras, models are created by defining a directed acyclic graph (DAG) of layers, where the layers represent different transformations applied to the input data. Each layer takes one or more inputs and produces one or more outputs, and these inputs and outputs are connected to each other to create the model architecture.\n",
    "\n",
    "The key concept in the functional API is that the graph of layers is used to define the relationships between inputs and outputs, and this graph can be reused to create multiple models. Each model can be created by specifying different inputs and outputs from the same graph of layers.\n",
    "\n",
    "Let's break down the explanation:\n",
    "\n",
    "Defining a Graph of Layers: In the functional API, we define a graph of layers by creating instances of different layer classes and connecting them together. The input to the graph is usually represented by an Input layer, and the output is usually obtained from one or more layers in the graph.\n",
    "\n",
    "Multiple Models: Once we have defined the graph of layers, we can create multiple models by specifying different inputs and outputs. We can reuse the same graph to create different models with various input-output configurations.\n",
    "\n",
    "Shared Weights: When we use the same graph of layers to create multiple models, the weights of the layers are shared among these models. This means that if we train one model using a particular set of inputs and outputs, the weights learned during training will be shared when we use the same layers to create another model with different inputs and outputs. This is particularly useful in transfer learning scenarios, where we can fine-tune a pre-trained model for a different task.\n",
    "\n",
    "By creating models based on a single graph of layers, the functional API provides a more flexible and modular way of defining complex architectures with shared layers, multiple inputs, and multiple outputs. This allows for efficient experimentation and reusability of model components, making it easier to develop and maintain sophisticated deep learning architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "https://www.geeksforgeeks.org/cifar-10-image-classification-in-tensorflow/\n",
    "\n",
    "Here we can see we have 5000 training images and 1000 test images\n",
    "as specified above and all the images are of 32 by 32 size\n",
    "and have 3 color channels i.e. images are color images. \n",
    "As well as it is also visible that there is only a single label assigned with each image.\n",
    "\n",
    "Loaging dataset\n",
    "\n",
    "'''\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_test.shape == (10000, 1)\n",
    "\n",
    "# Reduce pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    " \n",
    "# flatten the label values\n",
    "y_train, y_test = y_train.flatten(), y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# visualize data by plotting images\n",
    "\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "k = 0\n",
    " \n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i][j].imshow(x_train[k], aspect='auto')\n",
    "        k += 1\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "K = len(set(y_train))\n",
    " \n",
    "# calculate total number of classes\n",
    "# for output layer\n",
    "print(\"number of classes:\", K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the model using the functional API\n",
    "# Encoder Part\n",
    "\n",
    "# creates an input layer with the shape (32, 32, 3), representing grayscale images of size 32x32 pixels.\n",
    "i = layers.Input(shape = x_train[0].shape)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(i)    # applies a 2D convolutional layer with 32 filters of size 3x3 to the input.\n",
    "                                                                # The relu activation function is used.\n",
    "\n",
    "x = layers.BatchNormalization()(x) # applies BatchNormalization to the input data\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)    # Another 2D convolutional layer is applied with 32 filters of size 3x3 to \n",
    "                                                                # the output of the previous layer\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x) # A 2D max pooling layer with pool size 2x2 is applied to reduce the spatial dimensions of the data.\n",
    "\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the Encoder (Compressed Representation)\n",
    "encoder_output = layers.Flatten()(x)\n",
    "\n",
    "# Encoder Model\n",
    "encoder = keras.Model(i, encoder_output, name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Part\n",
    "x = layers.Dense(1024, activation='relu')(encoder_output)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(2048, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(8192, activation='relu')(x)\n",
    "x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "decoder_output = layers.Conv2DTranspose(1, (3, 3), activation='relu', padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Model\n",
    "decoder = keras.Model(encoder_output, decoder_output, name=\"decoder\")\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder_output = decoder(encoder_output)\n",
    "autoencoder = keras.Model(i, autoencoder_output, name=\"autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the autoencoder\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(encoder, 'encoder_compress.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(autoencoder, 'autoencoder_compress.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling models\n",
    "\n",
    "'''\n",
    "This is the step to define weigths\n",
    "If you do not compile metrics or evaluate your model, you'll receive a warning:\n",
    "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built.\n",
    "`model.compile_metrics` will be empty until you train or evaluate the model.\n",
    "'''\n",
    "\n",
    "encoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit the autoencoder model to reconstruct input\n",
    "history = encoder.fit(x_train, y_train, epochs=10, batch_size=32, verbose=2, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a\n",
    "# https://www.projectpro.io/recipes/run-and-fit-data-with-keras-model\n",
    "# https://machinelearningmastery.com/how-to-load-convert-and-save-images-with-the-keras-api/\n",
    "# https://keras.io/examples/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "\n",
    "encoder.save(\"functional_autoenconder_model.keras\")\n",
    "autoencoder.save(\"functional_enconder_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the example above, the stack of layers instantiate two models:\n",
    "- an encoder model that turns image inputs into 16-dimensional vectors, \n",
    "- and an end-to-end autoencoder model for training.\n",
    "\n",
    "Bellow, there's a different take on the autoencoder example that creates an encoder model, \n",
    "a decoder model, and chains them in two calls to obtain the autoencoder model:\n",
    "\n",
    "'''\n",
    "\n",
    "# 1st model\n",
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# 2nd model\n",
    "decoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\n",
    "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "encoded_img = encoder(autoencoder_input)\n",
    "decoded_img = decoder(encoded_img)\n",
    "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code defines an autoencoder model using the Keras functional API.\n",
    "In the given code, encoder(autoencoder_input) treats the encoder model as a function \n",
    "that takes the autoencoder_input tensor as input and produces the encoded_img tensor as output. \n",
    "It applies the encoder model to the input data to obtain the compressed representation in the latent space.\n",
    "\n",
    "The code defines an autoencoder architecture, consisting of an encoder part followed by a decoder part. \n",
    "The autoencoder is a type of neural network used for unsupervised learning, where it attempts to \n",
    "reconstruct the input data at its output, thus learning a compressed representation (latent space) of the input data. \n",
    "The encoder part compresses the input data into a lower-dimensional representation, and the decoder part tries to \n",
    "reconstruct the original data from this representation.\n",
    "\n",
    "'''\n",
    "\n",
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\") #creates an input layer for the autoencoder model.\n",
    "\n",
    "encoded_img = encoder(autoencoder_input)    # The code passes the autoencoder_input through the previously defined encoder model (encoder).\n",
    "                                            # This operation compresses the input images into a lower-dimensional representation (latent space) \n",
    "                                            # using the encoder.\n",
    "\n",
    "decoded_img = decoder(encoded_img)  # The code then takes the output of the encoder (encoded_img) \n",
    "                                    # and passes it through the previously defined decoder model (decoder). \n",
    "                                    # This operation reconstructs the original images from the compressed representation \n",
    "                                    # in the latent space using the decoder.\n",
    "\n",
    "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")   # The autoencoder takes the autoencoder_input and outputs \n",
    "                                                                                # the decoded_img, representing the reconstructed images. \n",
    "                                                                                # It creates an instance of the keras.Model \n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31012",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
